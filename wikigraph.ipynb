{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this notebook\n",
    "Throughout this notebook I will use pickle to save and load objects which take a long time to generate. [Read more about pickle](https://docs.python.org/3/library/pickle.html). This will save significant time by avoiding having to recreate these objects every time. If you choose to, you can [download the project files](https://adumb-files.s3.us-west-2.amazonaws.com/project_files.zip) I created in order to load the objects from these files. If you prefer, you can also generate these objects from scratch. At each stage, you will have the option to load the object from the file or create the object and then save it to a file. You will see where there are some places where you may prefer to use the pre generated objects, and places where you may want to create these objects from scratch in order to customize them.\n",
    "\n",
    "The notebook goes in order of how to create the graph from scratch. However, if you are using the project files and just want to generate the visual representation of the graph, you simply need to run these four cells: [Import](#import), [Load layout](#layout), [Load pre-styled graph](#styled_graph), and [Visualize graph](#visualize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports <a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import pickle\n",
    "import colorsys\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove self links and duplicate links from links file\n",
    "***NOTE: If you are using the project files then you can skip this step. If you scraped the Wikipedia dumps to generate the links_raw.csv file then you need to do this step.***\n",
    "\n",
    "Sometimes Wikipedia pages will link to themselves, or they will contain multiple links to the same article. This is not the behavior we want when graphing the articles, so we will create a refined links csv which does not have any duplicate or self links. This will also allow us to identify disguised dead ends, articles which appeared to have a link, but end up being dead ends once their self links are removed. We will append the disguised dead ends to the dead ends file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disguised_deadends = []\n",
    "\n",
    "with open('links_raw.csv') as links_file:\n",
    "    with open ('links.csv', 'w') as new_links_file:\n",
    "        reader = csv.reader(links_file)\n",
    "        next(reader, None)\n",
    "        writer = csv.writer(new_links_file)\n",
    "        writer.writerow(['source', 'target'])\n",
    "        \n",
    "        cur_page = ''\n",
    "        cur_links = set()\n",
    "        # Iterate over every link in the csv\n",
    "        for row in reader:\n",
    "            if cur_page != row[0]:\n",
    "                if len(cur_links) == 0:\n",
    "                    disguised_deadends.append(cur_page)\n",
    "                cur_links.clear()\n",
    "                cur_page = row[0]\n",
    "        \n",
    "            # Skip if duplicate link\n",
    "            if row[1] in cur_links:\n",
    "                continue\n",
    "            # Skip if self link\n",
    "            if row[1] == row[0]:\n",
    "                continue\n",
    "            \n",
    "            # Write link to new csv\n",
    "            cur_links.add(row[1])\n",
    "            writer.writerow([row[0], row[1]])\n",
    "\n",
    "# Remove first element of disguised_deadends since it will be an empty string\n",
    "disguised_deadends.pop(0)\n",
    "print(disguised_deadends)\n",
    "                    \n",
    "with open('deadends.txt', 'a') as deadends_file:\n",
    "    for page in disguised_deadends:\n",
    "        deadends_file.write(f'\\n{page}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associate each page with a unique ID\n",
    "We will create a bidirectional dictionary which will associate every page with a unique ID and allow us to look up the ID given the page name and vice versa.\n",
    "### Load page_ids dictionary from file using pickle\n",
    "If you are using the project files, you can run this line, otherwise you will need to create and save the page_ids dictionary before you can run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page_ids.pkl', 'rb') as page_file:\n",
    "    page_ids = pickle.load(page_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the page_ids dictionary\n",
    "You only need to do this if you haven't already loaded the page_ids dictionary from a saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count = 0\n",
    "page_ids = {}\n",
    "with open('links.csv') as links_file:\n",
    "    reader = csv.reader(links_file)\n",
    "    next(reader, None)\n",
    "\n",
    "    for row in reader:\n",
    "        # Add any page to the dictionary which has not already been found\n",
    "        if row[0] not in page_ids:\n",
    "            page_ids[row[0]] = page_count\n",
    "            page_ids[page_count] = row[0]\n",
    "            page_count += 1\n",
    "        if row[1] not in page_ids:\n",
    "            page_ids[row[1]] = page_count\n",
    "            page_ids[page_count] = row[1]\n",
    "            page_count += 1\n",
    "\n",
    "        # Print progress\n",
    "        if page_count % 10_000 == 0:\n",
    "            print(f'\\rPages processed: {page_count}', end='')\n",
    "\n",
    "    print()\n",
    "    print(page_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save page_ids dictionary to file\n",
    "You only need to run this once after creating the dictionary. Afterwards you should load the dictionary from the saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page_ids.pkl', 'wb') as page_file:\n",
    "    pickle.dump(page_ids, page_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list of edges for the graph\n",
    "Create an edge for every link in the csv file. These edges will be used to create the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "\n",
    "with open('links.csv') as links_file:\n",
    "    reader = csv.reader(links_file)\n",
    "    next(reader, None)\n",
    "\n",
    "    link_count = 0\n",
    "    for row in reader:                    \n",
    "        edges.append((page_ids[row[0]], page_ids[row[1]]))\n",
    "        \n",
    "        # Print progress\n",
    "        link_count += 1\n",
    "        if link_count % 100_000 == 0:\n",
    "            print(f'\\rLinks processed: {link_count}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the graph object\n",
    "We can now create the graph. This is all you need if you want to start doing analysis of the graph, like finding the shortest path between two objects. However, if you want to create a visual representation of the graph, more work is still needed.\n",
    "### Load graph using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph.pkl', 'rb') as graph_file:\n",
    "    graph = pickle.load(graph_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph from edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = int(len(page_ids)/2)\n",
    "graph = ig.Graph(node_count, edges=edges, directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph.pkl', 'wb') as graph_file:\n",
    "    pickle.dump(graph, graph_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the layout for the graph\n",
    "We will now need to generate the layout for the graph. The layout determines where each node will be placed on the graph. The layout algorithm I used is the Distributed Recursive Layout (DRL), which will attempt to cluster closely linked articles together. [Read more about the layout algorithms](https://python.igraph.org/en/stable/tutorial.html#layout-algorithms)\n",
    "\n",
    "***Note: This is probably the most resource intensive part of the entire project. It took about 4-5 days for the layout to finish generating***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load layout using pickle <a id='layout'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('layout.pkl', 'rb') as layout_file:\n",
    "    layout = pickle.load(layout_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate layout from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = graph.layout('drl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the layout object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('layout.pkl', 'wb') as layout_file:\n",
    "    pickle.dump(layout, layout_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect communitites within the graph\n",
    "At this point we could create the visual representation of the graph, but first we should make it more interesting by coloring each of the communities within the graph a different color. To do this, we first need to detect the diffent communities within the graph. This will be done using the [Leiden algortithm](https://leidenalg.readthedocs.io/en/stable/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load community list using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('partition_list.pkl', 'rb') as partition_file:\n",
    "    partition = pickle.load(partition_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate communities from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = la.find_partition(graph, la.ModularityVertexPartition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create community list\n",
    "Turns the partition object into a python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_list = []\n",
    "for p in partition:\n",
    "    partition_list.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the partition object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('partition_list.pkl', 'wb') as partition_file:\n",
    "    pickle.dump(partition_list, partition_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create community dictionary\n",
    "This will allow us to look up which community each article is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_dict = {}\n",
    "for i in range(len(partition)):\n",
    "    for v in partition[i]:\n",
    "        partition_dict[v] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style the nodes\n",
    "Now that we've determined which community each article belongs to, we can color the nodes of the graph according to their corresponding community. We will also resize each node according to the number of incoming links to its corresponding article using `indegree()`. The formulas to calculate the size and border width of each node were determined through a lot of trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-styled graph <a id='styled_graph'></a>\n",
    "If you load the pre-styled graph then you can skip the steps for styling the nodes and edges of the graph and skip to [visualizing the graph](#visualize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph_styled.pkl', 'rb') as graph_file:\n",
    "    graph = pickle.load(graph_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create different colors for each community\n",
    "This cell creates 44 different colors for each of the 44 communities. If you generate the communities from scratch, it may result in more or less than 44 communities, in which case you should adjust this cell as necessary. The colors were determined through trial and error to try and avoid very similar colors from being next to each other. The colors were first defined in hsv (hue, saturation, value) and then converted to rgb. The opacity of the verticies are 80% (0.8) and the edges are 5% (0.05). The reason for the low edge opacity is because of the large number of edges, otherwise it would be too cluttered to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_colors = []\n",
    "edge_colors = []\n",
    "hues = [262, 229, 196, 164, 131, 98, 65, 33, 0, 327, 295]\n",
    "saturations = [300, 567, 700, 433]\n",
    "for s in saturations:\n",
    "    for h in hues:\n",
    "        hue = h/359\n",
    "        saturation = s/1000\n",
    "        rgb_color = colorsys.hsv_to_rgb(hue, saturation, 1)\n",
    "        vertex_colors.append(f'rgba({round(rgb_color[0]*255)}, {round(rgb_color[1]*255)}, {round(rgb_color[2]*255)}, 0.8)')\n",
    "        edge_colors.append(f'rgba({round(rgb_color[0]*255)}, {round(rgb_color[1]*255)}, {round(rgb_color[2]*255)}, 0.05)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set color and size of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(partition)):\n",
    "    for vertex in partition[i]:\n",
    "        graph.vs[vertex]['color'] = vertex_colors[i]\n",
    "        graph.vs[vertex]['frame_color'] = 'rgba(0, 0, 0, 0.5)'\n",
    "        \n",
    "        size = 168 * math.log10(0.00005 * graph.vs[vertex].indegree() + 1) + 3\n",
    "        graph.vs[vertex]['size'] = size\n",
    "        frame_size = 0.1 + (6.56/197) * (size-3)\n",
    "        graph.vs[vertex]['frame_width'] = frame_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify the graph\n",
    "Now that we have calculated the layout, communities, and size of the nodes, let's simplify the graph. The graph has nearly 200,000,000 links, which will look horribly cluttered when visualized. It will also take a very long time to create the visualization. To simplify the graph, I only graphed 10% of the links and also removed links which go from one community to a different community. You can experiment with exactly how many links you want to graph and if you want to keep links that go between communities. However, from my testing, the best way to visualize the structure of the graph without too much clutter is to reduce the number of edges that are drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove edges from graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.delete_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select edges to add back to graph\n",
    "This is where you could change the code to visualize different data. For example if you wanted to only visualize the edges in community #3, you could do:\n",
    "```\n",
    "if source_partition == 2 and target_partition == 2:\n",
    "    edges.append((page_ids[row[0]], page_ids[row[1]]))\n",
    "```\n",
    "Or only graph edges that link to a specific article like the *United States*:\n",
    "```\n",
    "if row[1] == 'United States':\n",
    "    edges.append((page_ids[row[0]], page_ids[row[1]]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "\n",
    "with open('links.csv') as links_file:\n",
    "    reader = csv.reader(links_file)\n",
    "    next(reader, None)\n",
    "\n",
    "    link_count = 0\n",
    "    for row in reader:\n",
    "        # Print progress\n",
    "        link_count += 1\n",
    "        if link_count % 100_000 == 0:\n",
    "            print(f'\\rLinks processed: {link_count}', end='')\n",
    "            \n",
    "        source_partition = partition_dict[page_ids[row[0]]]\n",
    "        target_partition = partition_dict[page_ids[row[1]]]\n",
    "\n",
    "        # Only add edge if it starts and ends in the same community\n",
    "        if source_partition == target_partition:\n",
    "            # 10% chance to add link\n",
    "            rand = random.randint(0, 99)\n",
    "            if rand < 10:\n",
    "                edges.append((page_ids[row[0]], page_ids[row[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add edges back to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_edges(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style the edges\n",
    "Adds color to the edges according to the partition they are in.\n",
    "\n",
    "If you wanted, you could also change the colors of the nodes here by doing something like:\n",
    "```\n",
    "source_node = graph.es[edge].source\n",
    "target_node = graph.es[edge].target\n",
    "graph.vs[source_node]['color'] = 'rgba(100, 100, 100, 0.8)'\n",
    "graph.vs[target_node]['color'] = 'rgba(200, 200, 200, 0.8)'\n",
    "```\n",
    "This can be useful when you are graphing all the links to a specific article and you want the source nodes to be the same color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in range(graph.ecount()):\n",
    "    source_partition = partition_dict[graph.es[edge].source]\n",
    "    target_partition = partition_dict[graph.es[edge].target]\n",
    "    if source_partition == target_partition:\n",
    "        graph.es[edge]['color'] = edge_colors[source_partition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the styled graph\n",
    "The graph has now been fully styled and the particular styling of this graph can be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph_styled.pkl', 'wb') as graph_file:\n",
    "    pickle.dump(graph, graph_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the graph  <a id='visualize'></a>\n",
    "Create a png image of the graph. This image will be very large (10800 x 19200 pixels) in order to be able to zoom in on the image and see each dot. This image will also be in a portrait orientation and should be rotated counterclockwise 90° after the image has been generated. The reasoning is that it looked better this way, but you can also choose to generate the image in landscape orientation by changing the bounding box to 19200 x 10800. [Read more about styling the graph](https://python.igraph.org/en/stable/tutorial.html#drawing-a-graph-using-a-layout).\n",
    "\n",
    "***Note: The time to create the image varies on the number of edges you are trying to graph. If you try to graph all 200,000,000 edges it could take up to a day. Visualizing 10% of the edges could take about an hour***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_style = {}\n",
    "\n",
    "visual_style[\"bbox\"] = (10800, 19200) # Dimension of the image\n",
    "visual_style[\"margin\"] = 50 # Margin of pixelsfrom the borders\n",
    "visual_style[\"layout\"] = layout\n",
    "visual_style[\"background\"] = 'rgba(0, 0, 0, 0)' # Transparent background\n",
    "visual_style[\"edge_arrow_size\"] = 0\n",
    "visual_style[\"edge_width\"] = 0.25\n",
    "visual_style[\"vertex_order_by\"] = 'size' # Draw nodes in order of their size, larger nodes will be drawn last\n",
    "\n",
    "plot = ig.plot(graph, 'graph.png', **visual_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph analysis\n",
    "You should now have all the tools needed to visualize the graph and customize the graph to visualize particular data. The rest of the notebook will explain how to analyze the graph to extract certain data from the graph, similar to what was presented in the video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load graph with links\n",
    "For the graph analysis you will need the graph will all of the links. Using the styled graph here will give inaccurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph.pkl', 'rb') as graph_file:\n",
    "    graph = pickle.load(graph_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community categories\n",
    "Extract the top categories for a given community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create category dictionary\n",
    "This will create a dictionary that will allow us to look up all of the categories for a given page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories.csv') as links_file:\n",
    "    reader = csv.reader(links_file)\n",
    "    next(reader, None)\n",
    "\n",
    "    category_count = 0\n",
    "    categories = {}\n",
    "    for row in reader:\n",
    "        if row[0] not in page_ids:\n",
    "            continue\n",
    "        page_id = page_ids[row[0]]\n",
    "        if page_id in categories:\n",
    "            categories[page_id].append(row[1])\n",
    "        else:\n",
    "            categories[page_id] = [row[1]]\n",
    "        category_count += 1\n",
    "\n",
    "        if category_count % 100_000 == 0:\n",
    "            print(f'\\rCategories processed: {category_count}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create partition category dictionary\n",
    "This will create a dictionary for a specific community specified by `partition_idx`. This dictionary will count how many times each category appears in the given community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_categories = {}\n",
    "partition_idx = 6\n",
    "for page_id in partition[partition_idx]:\n",
    "    if page_id not in categories:\n",
    "        continue\n",
    "    page_categories = categories[page_id]\n",
    "    for category in page_categories:\n",
    "        if category in partition_categories:\n",
    "            partition_categories[category] += 1\n",
    "        else:\n",
    "            partition_categories[category] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the top 100 categories\n",
    "Prints the top 100 categories for the given community as well as the number of times each category appears in the community. You can adjust the code to print more or less than the top 100 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_categories = sorted(partition_categories.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_categories[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze incoming links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incoming links to a specific article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.vs[page_ids['United States']].indegree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create links_to_page dictionary\n",
    "This dictionary will keep track of how many incoming links each page has. We will then sort the dictionary to get the most linked articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_page = {}\n",
    "for vertex in range(graph.vcount()):\n",
    "    links_to_page[page_ids[vertex]] = graph.vs[vertex].indegree()\n",
    "    \n",
    "sorted_links = sorted(links_to_page.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 articles with most incoming links\n",
    "Use the sorted link count to print the top 100 most linked to articles as well as how many times they are linked to. You can adjust the code to print more or less than the top 100 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page, count in sorted_links[:100]:\n",
    "    print(f'{page.ljust(50, \" \")} {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dead ends & Orphans\n",
    "We already have a list of dead ends in the `deadends.txt` file which was found when scraping the Wikipedia dumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find orphans\n",
    "Note: this will not include dead end orphans since they are not on the graph. The list of dead end orphans should be appeneded to the list of orphans found in this cell to get a list of all dead end orphans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orphans = []\n",
    "for page in links_to_page:\n",
    "    if links_to_page[page] == 0:\n",
    "        orphans.append(page)\n",
    "\n",
    "print(len(orphans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dead end orphans\n",
    "These compares the list of dead ends to the articles in the graph. Since dead end orphans are not in the graph, any dead ends which are not in the graph are therefore dead end orphans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deadend_orphans = []\n",
    "with open('deadends.txt') as deadends_file:\n",
    "    for line in deadends_file:\n",
    "        title = line.strip()\n",
    "        if not title in links_to_page:\n",
    "            deadend_orphans.append(title)\n",
    "            \n",
    "print(deadend_orphans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths between articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a shortest path between two articles\n",
    "Finds a single path whose distance is the shortest between the two articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = page_ids['Hairy ball theorem']\n",
    "end = page_ids['Pepsi fruit juice flood']\n",
    "\n",
    "shortest_paths = graph.get_shortest_paths(start, to=end, output='vpath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all shortest paths between two articles\n",
    "Alternatively, this will find every path between two articles whose distances are the shortest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_paths = graph.get_all_shortest_paths(start, to=end, mode='out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print shortest paths\n",
    "This will print the shortest paths found in a human readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in shortest_paths:\n",
    "    for i in range(len(path)-1):\n",
    "        print(page_ids[path[i]], end=\" -> \")\n",
    "    print(page_ids[path[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph shortest path\n",
    "If you wanted to graph the shortest path, you should first remove all edges from the graph and color every node to be 100% transparent. Then you can run something like the following. Then you would create the image of the graph in the same way, but without setting the `edge_width` property on the `visual_style`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vertex in shortest_paths[0]:\n",
    "    graph.vs[vertex]['color'] = 'rgba(229, 145, 255, 1)'\n",
    "    graph.vs[vertex]['size'] = 150\n",
    "    graph.vs[vertex]['frame_width'] = 5\n",
    "    graph.vs[vertex]['frame_color'] = 'rgba(0, 0, 0, 0.8)'\n",
    "\n",
    "shortest_edges = []\n",
    "for i in range(1, len(shortest_paths[0])):\n",
    "    shortest_edges.append((shortest_paths[0][i-1], shortest_paths[0][i]))\n",
    "    \n",
    "graph.add_edges(shortest_edges)\n",
    "\n",
    "for edge in range(graph.ecount()):\n",
    "    graph.es[edge]['color'] = 'rgba(252, 255, 56, 1)'\n",
    "    graph.es[edge]['width'] = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degrees of separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create page_link_dict\n",
    "This will create a dictionary which will allow us to look up the list of outgoing links from a given page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_link_dict = {}\n",
    "\n",
    "link_count = 0\n",
    "with open('links.csv') as links_file:\n",
    "    reader = csv.reader(links_file)\n",
    "    next(reader, None)\n",
    "\n",
    "    for row in reader:\n",
    "        if row[1] not in page_link_dict:\n",
    "            page_link_dict[row[1]] = []\n",
    "            \n",
    "        if row[0] in page_link_dict:\n",
    "            page_link_dict[row[0]].append(row[1])\n",
    "        else:\n",
    "            page_link_dict[row[0]] = [row[1]]\n",
    "\n",
    "        link_count += 1\n",
    "        if link_count % 100_000 == 0:\n",
    "            print(f'Links processed: {link_count}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find pages in each degree\n",
    "Choose a starting page and then determine which pages are in each degree of separation starting from the 1st degree. You can change `degrees` to determine how many degrees of separation to go to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page = 'Pluto'\n",
    "degrees_of_separation = {start_page: 0}\n",
    "degrees = 10\n",
    "\n",
    "for degree in range(1, degrees+1):\n",
    "    print(f'Degree: {degree}')\n",
    "    for page in page_link_dict:\n",
    "        if page in degrees_of_separation and degrees_of_separation[page] == degree - 1:\n",
    "            for link in page_link_dict[page]:\n",
    "                if link not in degrees_of_separation:\n",
    "                    degrees_of_separation[link] = degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count articles in each degree of separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_counts = [0 for _ in range(degrees+1)]\n",
    "for page in degrees_of_separation:\n",
    "    degree_counts[degrees_of_separation[page]] += 1\n",
    "print(degree_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize degrees of separation\n",
    "If you wanted to visualize the degrees of separation, you should first remove all edges from the graph and color every node to be 100% transparent. Then you could run something like the following. Then you would create the image of the graph in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_colors = ['rgba(255, 77, 77, 0.8)', 'rgba(219, 255, 77, 0.8)', 'rgba(77, 255, 148, 0.8)', 'rgba(77, 148, 255, 0.8)', 'rgba(219, 77, 255, 0.8)', 'rgba(255, 184, 77, 0.8)', 'rgba(112, 255, 77, 0.8)', 'rgba(77, 255, 255, 0.8)', 'rgba(113, 77, 255, 0.8)', 'rgba(255, 77, 184, 0.8)']\n",
    "for vertex in range(graph.vcount()):\n",
    "    if page_ids[vertex] in degrees_of_separation:\n",
    "        degree = degrees_of_separation[page_ids[vertex]]\n",
    "        graph.vs[vertex]['color'] = degree_colors[degree-1]\n",
    "        graph.vs[vertex]['frame_color'] = 'rgba(0, 0, 0, 0.5)'\n",
    "        graph.vs[vertex]['size'] = 5\n",
    "        graph.vs[vertex]['frame_width'] = 0.16\n",
    "graph.vs[page_ids['Pluto']]['color'] = 'rgba(255, 77, 184, 0.8)'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
